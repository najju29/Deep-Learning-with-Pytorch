{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoGrad.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUI4tWXQqKUP"
      },
      "source": [
        "# Importing PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU5-yF8Kp-u4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn # It has almost all Layers and Activation functions.\n",
        "import torch.optim as optim # It has a lot of Optimizers.\n",
        "import torch.nn.functional as F # It has some Activation Functions as well.\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QURLW01FTA5P"
      },
      "source": [
        "# Importing Other Important Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I6UpUY_Uo04"
      },
      "source": [
        "# What is AutoGrad ?\n",
        "- <img src='https://miro.medium.com/max/1000/1*7w9w0tGu5pPSBeS6FbbzEQ.png' width=400 />\n",
        "\n",
        "- This class is an engine to calculate derivatives (Jacobian-vector product to be more precise). It records a graph of all the operations performed on a gradient enabled tensor and creates an acyclic graph called the dynamic computational graph. \n",
        "\n",
        "# Why do we need AutoGrad ?\n",
        "- After a Forward Propagation Neural Networks barely learns anything any thing and then it improves its performance using Optimization methods.\n",
        "- <img src='http://media5.datahacker.rs/2021/01/11-1024x593.jpg' width=400 />\n",
        "- All of the models uses Optimization techniques to improve the solution so as Neural Networks. We have bunch of different Optimization techniques available for Optimizing Neural Networks.\n",
        "- The Most commonly KNown one is *Gradient Descent* which iteratively updates the weights such that the loss is penalized in succesive steps. And these steps are decided by a parameter called as *Learning Rate*.\n",
        "- <img src='https://miro.medium.com/max/1400/1*tQTcGTLZqnI5rp3JYO_4NA.png' width=800 />\n",
        "- This Task is carried away by Complex Calculus calculations which infact is much harder to implement from scratch when we have huge number of parameter's weights to be optimized.\n",
        "- Luckily for us AutoGrad Package in Pytorch is there to do all those complex calculations by abstracting it and making it so easy to implement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl8ezDQrdn2X"
      },
      "source": [
        "# Let us Explore this stuff using an Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpoZgMfAdkoa",
        "outputId": "342864d2-69cb-49b4-c9df-e6aa571ffc64"
      },
      "source": [
        "# Let us initialize some imaginary Weights.\n",
        "Input_tensor = torch.tensor([2.,2.,2.,2.], requires_grad=True)\n",
        "\n",
        "# Looping through Epochs.\n",
        "for epoch in range(3):\n",
        "  # This would be out model's Desired output.\n",
        "  weight = 3\n",
        "  desired_model_output = (Input_tensor*weight).sum()\n",
        "  print(desired_model_output)\n",
        "  # Doing backward Propagation.\n",
        "  desired_model_output.backward()\n",
        "  print(Input_tensor.grad)\n",
        "  Input_tensor.grad.zero_()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(24., grad_fn=<SumBackward0>)\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor(24., grad_fn=<SumBackward0>)\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor(24., grad_fn=<SumBackward0>)\n",
            "tensor([3., 3., 3., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsFU37mJhJJt"
      },
      "source": [
        "## Observation:\n",
        "- As we can see the required weights to get that desired output is 3.\n",
        "- The weight with which we have multiplied our desired output was seen in backpropagation step.\n",
        "- This is how AutoGrad makes sure we reach out desired output using the optimum weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6wTJq9dkFas"
      },
      "source": [
        "# **THANK YOU**"
      ]
    }
  ]
}
