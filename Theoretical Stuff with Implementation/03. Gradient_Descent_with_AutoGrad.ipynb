{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient Descent with AutoGrad.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUI4tWXQqKUP"
      },
      "source": [
        "# Importing PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU5-yF8Kp-u4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn # It has almost all Layers and Activation functions.\n",
        "import torch.optim as optim # It has a lot of Optimizers.\n",
        "import torch.nn.functional as F # It has some Activation Functions as well.\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QURLW01FTA5P"
      },
      "source": [
        "# Importing Other Important Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yf59ehIrfa7"
      },
      "source": [
        "- Now let us try to see the underlying algorithm of how Gradient Descent works from Scratch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHyh-hT0UFNW"
      },
      "source": [
        "# What is Gradient Descent ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ2jGF5wWWul"
      },
      "source": [
        "- let us suppose we have **X=5 , Y_true = 10**.\n",
        "- Now the Objective is to find the value of **'W'** such that **W*X=Y_true**.\n",
        "- Now applying it here is pretty easy where we divide Y_true with X to give 10/5=2 as the value of **W**.\n",
        "- But in real world the equation is not that easy to solve and need **Exact Solution** to solve the equations.\n",
        "- There are many **Numerical Methods** in Mathematics to solve this kind of **Exact equations** of which one is **Gradient Descent** Algorithm.\n",
        "- **Gradient Descent:**Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In Deep learning, we use gradient descent to update the parameters of our model.\n",
        "-- <img src='https://lucidar.me/en/neural-networks/files/gradient-overview.png' width=400 />\n",
        "- Here Target minimum is known as **Global Minima** which is the lowest possible error between the true and predicted value.\n",
        "- At this Global Minima whatever weight it has applied to reach the point would be considered as an optium Weight for prediction.\n",
        "- We will initially use *Numpy for understanding it from depth*.\n",
        "- Later we will use **AUTOGRAD** for further simplification.\n",
        "- As we are gonna watch two implementations let us move forward with the first one.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJZukQMEsvyZ"
      },
      "source": [
        "# Gradient Descent using Numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5RyPHsiaBtI"
      },
      "source": [
        "## Initializing the Data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNUhoIBJkSdO"
      },
      "source": [
        "# Firstly let us decide our Predictor->X and Target->Y values respectively.\n",
        "X = np.array([1,2,3,4])\n",
        "Y = np.array([3,6,9,12])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOEaovnWXwIy"
      },
      "source": [
        "- The Equation of our forward step is **Y=W*X,** and clearly the value of **W** is **3** which upon multiplied to **X** will give us **Y**. \n",
        "- Let us intiate **W** to zero and see if we can iteratively reach to the original solution by reducing the weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbcr12NlkSg2"
      },
      "source": [
        "# Let us firstly set weight to zero\n",
        "W=0"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjnHA2iBZCHb"
      },
      "source": [
        "- Let us make our forward function as follows.\n",
        "- Also will try to make a loss function which in our case would be **Mean Squarred error.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Tmo15LyaG2X"
      },
      "source": [
        "## Creating Forward function and Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqAj0xOmkSkJ"
      },
      "source": [
        "def forward_step(X):\n",
        "  predicted_value = W*X\n",
        "  return predicted_value\n",
        "\n",
        "def loss_function(y_true, y_pred):\n",
        "  # Mean Squared Error.\n",
        "  loss = ((y_true-y_pred)**2).mean()\n",
        "  return loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWqrkxDsZnUR"
      },
      "source": [
        "## Gradient Function:\n",
        "- Our Gradient Function is always dependent on the Loss function that we choode.\n",
        "- Infact it is the **First Derivative** of our loss function.\n",
        "- **Loss function:**\n",
        "- <img src='https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTFC9BlOat6q3hSstsBxmrCweT6NDdfj6mB206wzGPAyM-FwlE7TsoU_Kb-qwyrKhHS5cs&usqp=CAU' width=400 height=100/>\n",
        "- **Loss function** after differentiating one time becomes the **objective function** or **Gradient function** and looks like follows:\n",
        "- <img src='https://hackernoon.com/hn-images/0*XFK9C3go0VaWR_f4.png' width=400 />\n",
        "- Using gradient function to update weights where formula looks like follows.\n",
        "- <img src='https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSKG9gm8Q1Xwaz-L-W_U0qNY02X7nENMLRJBUtY1eVOyZryQgYqcE_gnQpWhCiAPVmYKQ&usqp=CAU' width=400 />\n",
        "\n",
        "- where *Alpha* is the **Learning Rate**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jBW-jQTgupr"
      },
      "source": [
        "## Applying Gradient Descent using Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAvenUBEkSn1"
      },
      "source": [
        "# The gradient function after differentiation.\n",
        "def Gradient_function(X,y_true, y_pred):\n",
        "  return np.dot(2*X , y_pred-y_true).mean()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udrNXmjJkSq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "953bf166-3548-4257-e50f-f82fe64c6829"
      },
      "source": [
        "# Let us initiate some parameters and start finiding solution to this.\n",
        "W=0\n",
        "print(f'Initialized Weight :{W}')\n",
        "print(f'Predictions with this weight:{forward_step(X)}')\n",
        "learning_rate = 0.01\n",
        "n_iters = 11\n",
        "for i in range(n_iters):\n",
        "  print(f'Weight before forward:{W}')\n",
        "  y_preds = forward_step(X)\n",
        "  loss = loss_function(Y, y_preds)\n",
        "  gradient = Gradient_function(X, Y, y_preds)\n",
        "  W = W - gradient*learning_rate\n",
        "  print(f'Parameters at {i} steps: \\nPredictions={y_preds}, loss={loss:.3f}, updated weight={W:.3f}')\n",
        "print('\\n')\n",
        "y_preds = [round(i,3) for i in y_preds]\n",
        "print(f'Comparing real and predicted values\\n Y_true={Y}\\n Y_pred={y_preds}')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized Weight :0\n",
            "Predictions with this weight:[0 0 0 0]\n",
            "Weight before forward:0\n",
            "Parameters at 0 steps: \n",
            "Predictions=[0 0 0 0], loss=67.500, updated weight=1.800\n",
            "Weight before forward:1.8\n",
            "Parameters at 1 steps: \n",
            "Predictions=[1.8 3.6 5.4 7.2], loss=10.800, updated weight=2.520\n",
            "Weight before forward:2.52\n",
            "Parameters at 2 steps: \n",
            "Predictions=[ 2.52  5.04  7.56 10.08], loss=1.728, updated weight=2.808\n",
            "Weight before forward:2.808\n",
            "Parameters at 3 steps: \n",
            "Predictions=[ 2.808  5.616  8.424 11.232], loss=0.276, updated weight=2.923\n",
            "Weight before forward:2.9232\n",
            "Parameters at 4 steps: \n",
            "Predictions=[ 2.9232  5.8464  8.7696 11.6928], loss=0.044, updated weight=2.969\n",
            "Weight before forward:2.96928\n",
            "Parameters at 5 steps: \n",
            "Predictions=[ 2.96928  5.93856  8.90784 11.87712], loss=0.007, updated weight=2.988\n",
            "Weight before forward:2.987712\n",
            "Parameters at 6 steps: \n",
            "Predictions=[ 2.987712  5.975424  8.963136 11.950848], loss=0.001, updated weight=2.995\n",
            "Weight before forward:2.9950848\n",
            "Parameters at 7 steps: \n",
            "Predictions=[ 2.9950848  5.9901696  8.9852544 11.9803392], loss=0.000, updated weight=2.998\n",
            "Weight before forward:2.99803392\n",
            "Parameters at 8 steps: \n",
            "Predictions=[ 2.99803392  5.99606784  8.99410176 11.99213568], loss=0.000, updated weight=2.999\n",
            "Weight before forward:2.999213568\n",
            "Parameters at 9 steps: \n",
            "Predictions=[ 2.99921357  5.99842714  8.9976407  11.99685427], loss=0.000, updated weight=3.000\n",
            "Weight before forward:2.9996854272\n",
            "Parameters at 10 steps: \n",
            "Predictions=[ 2.99968543  5.99937085  8.99905628 11.99874171], loss=0.000, updated weight=3.000\n",
            "\n",
            "\n",
            "Comparing real and predicted values\n",
            " Y_true=[ 3  6  9 12]\n",
            " Y_pred=[3.0, 5.999, 8.999, 11.999]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l2fmGAIgd9p"
      },
      "source": [
        "- Looks Pretty Cool!\n",
        "- Now let us try the same using **Pytorch's AUTOGRAD** and lets see how simple it is to perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-FsdvpUgzUu"
      },
      "source": [
        "# Gradient Descent using Pytorch "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALBay63bqSi6"
      },
      "source": [
        "## Initializing data to Pytorch's Tensors and using them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH6uZ5PckSun"
      },
      "source": [
        "# Firstly we have to change the properties of our Variables. We will convert this into pytorch tensor as we will not consider any numpy array.\n",
        "# Using numpy array can cause error while doing operations of Gradients Calculation.\n",
        "\n",
        "X = torch.tensor([1,2,3,4], dtype=torch.float32) # Always specify the Data Type if possible.\n",
        "Y = torch.tensor([3,6,9,12], dtype=torch.float32)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H792A0VwkSym"
      },
      "source": [
        "# Also the weight should be a Torch tensor.\n",
        "\n",
        "W = torch.tensor([0.0], dtype=torch.float32, requires_grad=True) # Requires_grad = True because we are intrested in its gradient."
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzEks2yBjNmK"
      },
      "source": [
        "- Using the same Loss and Forward functions.\n",
        "- Let us move forward to the training part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izGiHpTwqYYq"
      },
      "source": [
        "## Applying Gradient Descent using Pytorch's AutoGrad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMt57jFfkS3G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85ed5ee1-3bd1-46e7-efec-59172a2b3d23"
      },
      "source": [
        "# Let us initiate some parameters and start finiding solution to this.\n",
        "learning_rate = 0.01\n",
        "n_iters = 11\n",
        "for i in range(n_iters):\n",
        "  print(f'Weight before forward:{W}')\n",
        "  y_preds = forward_step(X)\n",
        "  loss = loss_function(Y, y_preds)\n",
        "\n",
        "  #gradient = Gradient_function(X, Y, y_preds) **Not going to use this instead we have....\n",
        "\n",
        "  loss.backward()\n",
        "  print(W.grad)\n",
        "  # Updating the weights without want it to print.\n",
        "  # returns no gradient to visualize which gives us a pleasant view.\n",
        "  with torch.no_grad():\n",
        "    W -= (learning_rate * W.grad) # Multiplying it using gradient of Weight vector.\n",
        "  \n",
        "  print(W)\n",
        "  # Always clear the output of gradient so that it will not clash with the previous gradient.\n",
        "  W.grad.zero_()\n",
        "  if i%2==0:\n",
        "    print(f'Parameters at {i} steps: \\nPredictions={y_preds}, loss={loss}, updated weight={W}')\n",
        "print('\\n')\n",
        "print(f'Comparing real and predicted values\\n Y_true={Y}\\n Y_pred={y_preds}')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weight before forward:tensor([0.], requires_grad=True)\n",
            "tensor([-45.])\n",
            "tensor([0.4500], requires_grad=True)\n",
            "Parameters at 0 steps: \n",
            "Predictions=tensor([0., 0., 0., 0.], grad_fn=<MulBackward0>), loss=67.5, updated weight=tensor([0.4500], requires_grad=True)\n",
            "Weight before forward:tensor([0.4500], requires_grad=True)\n",
            "tensor([-38.2500])\n",
            "tensor([0.8325], requires_grad=True)\n",
            "Weight before forward:tensor([0.8325], requires_grad=True)\n",
            "tensor([-32.5125])\n",
            "tensor([1.1576], requires_grad=True)\n",
            "Parameters at 2 steps: \n",
            "Predictions=tensor([0.8325, 1.6650, 2.4975, 3.3300], grad_fn=<MulBackward0>), loss=35.23542022705078, updated weight=tensor([1.1576], requires_grad=True)\n",
            "Weight before forward:tensor([1.1576], requires_grad=True)\n",
            "tensor([-27.6356])\n",
            "tensor([1.4340], requires_grad=True)\n",
            "Weight before forward:tensor([1.4340], requires_grad=True)\n",
            "tensor([-23.4903])\n",
            "tensor([1.6689], requires_grad=True)\n",
            "Parameters at 4 steps: \n",
            "Predictions=tensor([1.4340, 2.8680, 4.3019, 5.7359], grad_fn=<MulBackward0>), loss=18.393112182617188, updated weight=tensor([1.6689], requires_grad=True)\n",
            "Weight before forward:tensor([1.6689], requires_grad=True)\n",
            "tensor([-19.9667])\n",
            "tensor([1.8686], requires_grad=True)\n",
            "Weight before forward:tensor([1.8686], requires_grad=True)\n",
            "tensor([-16.9717])\n",
            "tensor([2.0383], requires_grad=True)\n",
            "Parameters at 6 steps: \n",
            "Predictions=tensor([1.8686, 3.7371, 5.6057, 7.4742], grad_fn=<MulBackward0>), loss=9.601317405700684, updated weight=tensor([2.0383], requires_grad=True)\n",
            "Weight before forward:tensor([2.0383], requires_grad=True)\n",
            "tensor([-14.4260])\n",
            "tensor([2.1825], requires_grad=True)\n",
            "Weight before forward:tensor([2.1825], requires_grad=True)\n",
            "tensor([-12.2621])\n",
            "tensor([2.3051], requires_grad=True)\n",
            "Parameters at 8 steps: \n",
            "Predictions=tensor([2.1825, 4.3651, 6.5476, 8.7301], grad_fn=<MulBackward0>), loss=5.0119476318359375, updated weight=tensor([2.3051], requires_grad=True)\n",
            "Weight before forward:tensor([2.3051], requires_grad=True)\n",
            "tensor([-10.4228])\n",
            "tensor([2.4094], requires_grad=True)\n",
            "Weight before forward:tensor([2.4094], requires_grad=True)\n",
            "tensor([-8.8593])\n",
            "tensor([2.4980], requires_grad=True)\n",
            "Parameters at 10 steps: \n",
            "Predictions=tensor([2.4094, 4.8188, 7.2281, 9.6375], grad_fn=<MulBackward0>), loss=2.6162679195404053, updated weight=tensor([2.4980], requires_grad=True)\n",
            "\n",
            "\n",
            "Comparing real and predicted values\n",
            " Y_true=tensor([ 3.,  6.,  9., 12.])\n",
            " Y_pred=tensor([2.4094, 4.8188, 7.2281, 9.6375], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6IM2UgXpD2j"
      },
      "source": [
        "- **YAY!!!** got the same result but with some differences.\n",
        "- Let us **Increase the Epochs** and see the differences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSmcXBRZp4r0",
        "outputId": "f0b2b2cf-9cb4-4078-e4cd-6b61746757ce"
      },
      "source": [
        "# Let us initiate some parameters and start finiding solution to this.\n",
        "learning_rate = 0.01\n",
        "n_iters = 25\n",
        "for i in range(n_iters):\n",
        "  print(f'Weight before forward:{W}')\n",
        "  y_preds = forward_step(X)\n",
        "  loss = loss_function(Y, y_preds)\n",
        "\n",
        "  #gradient = Gradient_function(X, Y, y_preds) **Not going to use this instead we have....\n",
        "\n",
        "  loss.backward()\n",
        "  print(W.grad)\n",
        "  # Updating the weights without want it to print.\n",
        "  # returns no gradient to visualize which gives us a pleasant view.\n",
        "  with torch.no_grad():\n",
        "    W -= (learning_rate * W.grad) # Multiplying it using gradient of Weight vector.\n",
        "  \n",
        "  print(W)\n",
        "  # Always clear the output of gradient so that it will not clash with the previous gradient.\n",
        "  W.grad.zero_()\n",
        "  if i%5==0:\n",
        "    print(f'Parameters at {i} steps: \\nPredictions={y_preds}, loss={loss}, updated weight={W}')\n",
        "print('\\n')\n",
        "print(f'Comparing real and predicted values\\n Y_true={Y}\\n Y_pred={y_preds}')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weight before forward:tensor([0.], requires_grad=True)\n",
            "tensor([-45.])\n",
            "tensor([0.4500], requires_grad=True)\n",
            "Parameters at 0 steps: \n",
            "Predictions=tensor([0., 0., 0., 0.], grad_fn=<MulBackward0>), loss=67.5, updated weight=tensor([0.4500], requires_grad=True)\n",
            "Weight before forward:tensor([0.4500], requires_grad=True)\n",
            "tensor([-38.2500])\n",
            "tensor([0.8325], requires_grad=True)\n",
            "Weight before forward:tensor([0.8325], requires_grad=True)\n",
            "tensor([-32.5125])\n",
            "tensor([1.1576], requires_grad=True)\n",
            "Weight before forward:tensor([1.1576], requires_grad=True)\n",
            "tensor([-27.6356])\n",
            "tensor([1.4340], requires_grad=True)\n",
            "Weight before forward:tensor([1.4340], requires_grad=True)\n",
            "tensor([-23.4903])\n",
            "tensor([1.6689], requires_grad=True)\n",
            "Weight before forward:tensor([1.6689], requires_grad=True)\n",
            "tensor([-19.9667])\n",
            "tensor([1.8686], requires_grad=True)\n",
            "Parameters at 5 steps: \n",
            "Predictions=tensor([1.6689, 3.3378, 5.0067, 6.6755], grad_fn=<MulBackward0>), loss=13.289022445678711, updated weight=tensor([1.8686], requires_grad=True)\n",
            "Weight before forward:tensor([1.8686], requires_grad=True)\n",
            "tensor([-16.9717])\n",
            "tensor([2.0383], requires_grad=True)\n",
            "Weight before forward:tensor([2.0383], requires_grad=True)\n",
            "tensor([-14.4260])\n",
            "tensor([2.1825], requires_grad=True)\n",
            "Weight before forward:tensor([2.1825], requires_grad=True)\n",
            "tensor([-12.2621])\n",
            "tensor([2.3051], requires_grad=True)\n",
            "Weight before forward:tensor([2.3051], requires_grad=True)\n",
            "tensor([-10.4228])\n",
            "tensor([2.4094], requires_grad=True)\n",
            "Weight before forward:tensor([2.4094], requires_grad=True)\n",
            "tensor([-8.8593])\n",
            "tensor([2.4980], requires_grad=True)\n",
            "Parameters at 10 steps: \n",
            "Predictions=tensor([2.4094, 4.8188, 7.2281, 9.6375], grad_fn=<MulBackward0>), loss=2.6162679195404053, updated weight=tensor([2.4980], requires_grad=True)\n",
            "Weight before forward:tensor([2.4980], requires_grad=True)\n",
            "tensor([-7.5304])\n",
            "tensor([2.5733], requires_grad=True)\n",
            "Weight before forward:tensor([2.5733], requires_grad=True)\n",
            "tensor([-6.4009])\n",
            "tensor([2.6373], requires_grad=True)\n",
            "Weight before forward:tensor([2.6373], requires_grad=True)\n",
            "tensor([-5.4407])\n",
            "tensor([2.6917], requires_grad=True)\n",
            "Weight before forward:tensor([2.6917], requires_grad=True)\n",
            "tensor([-4.6246])\n",
            "tensor([2.7379], requires_grad=True)\n",
            "Weight before forward:tensor([2.7379], requires_grad=True)\n",
            "tensor([-3.9309])\n",
            "tensor([2.7772], requires_grad=True)\n",
            "Parameters at 15 steps: \n",
            "Predictions=tensor([ 2.7379,  5.4759,  8.2138, 10.9517], grad_fn=<MulBackward0>), loss=0.5150766968727112, updated weight=tensor([2.7772], requires_grad=True)\n",
            "Weight before forward:tensor([2.7772], requires_grad=True)\n",
            "tensor([-3.3413])\n",
            "tensor([2.8107], requires_grad=True)\n",
            "Weight before forward:tensor([2.8107], requires_grad=True)\n",
            "tensor([-2.8401])\n",
            "tensor([2.8391], requires_grad=True)\n",
            "Weight before forward:tensor([2.8391], requires_grad=True)\n",
            "tensor([-2.4141])\n",
            "tensor([2.8632], requires_grad=True)\n",
            "Weight before forward:tensor([2.8632], requires_grad=True)\n",
            "tensor([-2.0520])\n",
            "tensor([2.8837], requires_grad=True)\n",
            "Weight before forward:tensor([2.8837], requires_grad=True)\n",
            "tensor([-1.7442])\n",
            "tensor([2.9012], requires_grad=True)\n",
            "Parameters at 20 steps: \n",
            "Predictions=tensor([ 2.8837,  5.7674,  8.6512, 11.5349], grad_fn=<MulBackward0>), loss=0.10140542685985565, updated weight=tensor([2.9012], requires_grad=True)\n",
            "Weight before forward:tensor([2.9012], requires_grad=True)\n",
            "tensor([-1.4826])\n",
            "tensor([2.9160], requires_grad=True)\n",
            "Weight before forward:tensor([2.9160], requires_grad=True)\n",
            "tensor([-1.2602])\n",
            "tensor([2.9286], requires_grad=True)\n",
            "Weight before forward:tensor([2.9286], requires_grad=True)\n",
            "tensor([-1.0711])\n",
            "tensor([2.9393], requires_grad=True)\n",
            "Weight before forward:tensor([2.9393], requires_grad=True)\n",
            "tensor([-0.9105])\n",
            "tensor([2.9484], requires_grad=True)\n",
            "\n",
            "\n",
            "Comparing real and predicted values\n",
            " Y_true=tensor([ 3.,  6.,  9., 12.])\n",
            " Y_pred=tensor([ 2.9393,  5.8786,  8.8179, 11.7572], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEvItL87qFYu"
      },
      "source": [
        "- Increasing the number of Epochs does the Job for us, Now let us quickly jump to the Conclusions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKKdme_epPpc"
      },
      "source": [
        "# Conclusions:\n",
        "- The *Custom* one reached the real value **more faster** than the *Torch* one but still increasing the Epochs can give us the same result as well.\n",
        "- **AutoGrad reduces the headache of keeping track of gradients.**"
      ]
    }
  ]
}
