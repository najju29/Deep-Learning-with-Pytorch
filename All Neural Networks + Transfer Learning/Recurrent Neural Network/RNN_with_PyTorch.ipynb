{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_with_PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZuG-c3AGwxv"
      },
      "source": [
        "# What is Recurring in Maths ?\n",
        "- Recurring in Maths is simply the repettition of a decimal or an expression.\n",
        "- A recurrence relation is an equation that defines a sequence based on a rule that gives the next term as a function of the previous term(s).\n",
        "- The simplest form of a recurrence relation is the case where the next term depends only on the immediately previous term.\n",
        "\n",
        "# Recurrent Neural Networks(RNN) in Deep Learning.\n",
        "- Recurrent Neural Networks (RNN) are a class of Artificial Neural Networks that can process a sequence of inputs in deep learning and retain its state while processing the next sequence of inputs.\n",
        "- Traditional neural networks will process an input and move onto the next one disregarding its sequence.\n",
        "\n",
        "# **How RNN works:**\n",
        "- <img src=\"https://www.simplilearn.com/ice9/free_resources_article_thumb/Fully_connected_Recurrent_Neural_Network.gif\" width=\"800\" height=\"350\">\n",
        "- It uses previous information to affect later ones.\n",
        "-There are 3 layers: **Input(x), Hidden(h) and output(y).**\n",
        "- In forward step when an input is recieved into the cell it gets multiplied by an **initialized weight** and then it is been **sent as an output** and then **used as an input in the next neuron**.\n",
        "- Let us visualize it using a cell level diagram.\n",
        "- <img src=\"https://blog.floydhub.com/content/images/2019/06/ezgif.com-video-to-gif.gif\" width=\"500\" height=\"350\">\n",
        "- The output of other neurons gets weight initialization as well. Except the first neuron of each layer all other neurons get inputs from **two ends 1) from the previous neuron and 2) other as an input** i.e., next element in the data.\n",
        "- This is how any current neuron will have info of its current and previous data simultaneously.\n",
        "- *The loop*: passes the input forward sequentialy, while retaining information about it.\n",
        "- Here the information possesed by a neuron is passed on to the next one as well so that the new neuron will learn new pattern by keeping previous pattern in mind.\n",
        "- This is how the sequential information in the data is preserved.\n",
        "- Thus, It performs very good when it is given any sequential data of numeric or textual format.\n",
        "\n",
        "# Types of RNN\n",
        "- There are four types of RNNs\n",
        "- <img src=\"https://static.packt-cdn.com/products/9781789536089/graphics/b2e068f5-08f8-4e4a-b56c-e29675ab0eb5.png\" width=\"1100\" height=\"400\">\n",
        "\n",
        "# Understanding the Maths behind RNN\n",
        "- Let us follow up with this diagram of **Many to Many RNN** to understand more.\n",
        "- <img src=\"https://miro.medium.com/max/2967/1*7_pAvVIMNp8h2aI4sdWdLg.png\" width=\"800\" height=\"350\">\n",
        "- Here **a0** is the initialized hidden state, It is being initialized because when start of there is no hidden state available for the first hidden cell.\n",
        "- There is also an input **x0** comming into the the cell. Now both of this gets their own **weights initialized** and the hidden state becomes **Wha.a0** and the input becomes **Whx.x0**.\n",
        "- Both of this dot products gets added and becomes **(Wha.a0+Whx.x0)**. Now after adding a **Bais(bh)** vector the whole sum becomes **(Wha.a0+Whx.x0+bh)**.\n",
        "- Now this goes into an Activation function, **TanH** is most commonly used Activation function in RNNs as it keeps the range of values in between **range(-1,1)**.\n",
        "- Now the output is **F(Wha.a0+Whx.x0+bh)**, where **F** is Tanh activation function.\n",
        "- Let us visualize the whole above process in below GIF.\n",
        "- <img src=\"https://miro.medium.com/max/1400/1*WMnFSJHzOloFlJHU6fVN-g.gif\" width=\"800\" height=\"350\">\n",
        "- In above gif, the **new hidden state** is the output of the current hidden cell which can be regarded as **F(Wha.a0+Whx.x0+bh)** from the above math.\n",
        "- Now in the new hidden cell, this output recieved acts as an hidden state.\n",
        "- Same can be visualized in below gif.\n",
        "- <img src=\"https://miro.medium.com/max/1900/1*o-Cq5U8-tfa1_ve2Pf3nfg.gif\" width=\"900\" height=\"300\">\n",
        "- This is how RNNs are different from the ANNs, Thats all about RNNs let us proceed further to see its implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JBY08wO9SkC"
      },
      "source": [
        "- **NOTE:** Please refer to a more proper illustration about how RNN works before proceeding any further. The above ones are just simple tips about how different they are from the Artificial Neural Networks.\n",
        "- For Further info please go through this awesome blog:\n",
        "[RNN by Stanford](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks), and for video watch this simple illustration [RNN Illustration Video](https://www.youtube.com/watch?v=LHXXI4-IEns&t=496s&ab_channel=TheA.I.Hacker-MichaelPhiTheA.I.Hacker-MichaelPhi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc1q3L_YTZxi"
      },
      "source": [
        "# Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNDDckH5GuKH"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Reay1nUUDWg"
      },
      "source": [
        "# Let us Build a RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpj-VmVeUZVe"
      },
      "source": [
        "## Setting up **DATASET** and **HYPERPARAMETERS**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM5JpZrQUZHI"
      },
      "source": [
        "input_size = 28 # Image of 28x28 pixels which upon flattening becomes 784.\n",
        "sequence_length = 28\n",
        "num_of_layers = 2 # Number of RNN layers we want.\n",
        "hidden_size = 256 # Number of neuron per hidden state.\n",
        "num_classes = 10 # Number of classes in the Dataset are 10.\n",
        "learning_rate = 0.001 # Speed at which we want our optimizer to optimize our solution.\n",
        "batch_size = 64 # Size of the batch that will undergo training at one step.\n",
        "epochs = 2 # Steps of training or times a forward and backward propagation is done.\n",
        "\n",
        "# Firstly Loading a Data and downloading it to the folder.\n",
        "train_dataset = datasets.MNIST(root=\"content/\",train=True,\n",
        "                               transform=transforms.ToTensor(),download=True)\n",
        "# Now setting up its properties like batchsize\n",
        "trainloader = DataLoader(dataset=train_dataset,\n",
        "                         batch_size = batch_size,\n",
        "                         shuffle=True\n",
        "                         )\n",
        "\n",
        "# Doing the same for testset as well\n",
        "test_dataset = datasets.MNIST(root=\"content/\",train=False,\n",
        "                               transform=transforms.ToTensor(),download=True)\n",
        "\n",
        "testloader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size = batch_size,\n",
        "                         shuffle=True\n",
        "                         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpNGRYcBqMfT"
      },
      "source": [
        "## Creating RNN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKVX9o2aUZEI"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_of_layers, num_of_classes):\n",
        "    super(RNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_of_layers = num_of_layers\n",
        "    self.num_of_classes = num_of_classes\n",
        "    self.rnn = nn.RNN(input_size, hidden_size, num_of_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size*sequence_length, num_of_classes)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # Initializing hidden state.\n",
        "    h0 = torch.zeros(self.num_of_layers, x.size(0), self.hidden_size) # Initial Hidden State for each element in the batch.\n",
        "    # Forward step.\n",
        "    rnn_out_vector,_ = self.rnn(x, h0) # It takes in input vector followed by hidden state, returns and output vector along with hidden state.\n",
        "    rnn_out_vector = rnn_out_vector.reshape(rnn_out_vector.shape[0],-1)\n",
        "    output_from_linear_layer = self.fc(rnn_out_vector) # Sending in output of RNN into the Linear Layer(ANN)\n",
        "    return output_from_linear_layer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E6wlTB3wfLk"
      },
      "source": [
        "# Initializing Model, Loss, Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRxQ3U1yUY-S"
      },
      "source": [
        "# Initializing the Model\n",
        "model = RNN(input_size, hidden_size, num_of_layers, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6d05lMUGuGo"
      },
      "source": [
        "\n",
        "# Using CrossEntropyLoss as we have multiple classes.\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer as ADAM\n",
        "optimizer = optim.Adam(params=model.parameters(),lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIPtvxIMGnv5",
        "outputId": "65fcb1b5-a6d1-4915-cabf-47e91e549ef2"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  for batch_index, (data, targets) in enumerate(trainloader):# output of format = batch_index, (element_data, its_target)\n",
        "    data=data.squeeze(1)\n",
        "    # Forward Step\n",
        "     # Making Predictions on train_data\n",
        "    training_predictions = model(data)\n",
        "     # Calculating loss\n",
        "    Training_loss = loss_function(training_predictions, targets)\n",
        "    # Backward Step\n",
        "    optimizer.zero_grad()\n",
        "    Training_loss.backward()\n",
        "\n",
        "    # Optimizer Step\n",
        "    optimizer.step()\n",
        "  print(f'At {epoch} epochs Training_loss={Training_loss}')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "At 0 epochs Training_loss=0.0071024359203875065\n",
            "At 1 epochs Training_loss=0.2920049726963043\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ndh9Hps0K2F"
      },
      "source": [
        "# Custom Function that calcultes accuracy of the Model.\n",
        "\n",
        "def check_accuracy(loader, model):\n",
        "    if loader.dataset.train:\n",
        "        print(\"Checking accuracy on training data\")\n",
        "    else:\n",
        "        print(\"Checking accuracy on test data\")\n",
        "\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.squeeze(1)\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "        print(f\"Got {num_correct} / {num_samples} with accuracy = {float(num_correct)/float(num_samples)*100:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PcIBsvk7u5Z",
        "outputId": "4bf5a3b1-38d7-4505-987d-c4a34f7cc48f"
      },
      "source": [
        "model.train() # Training the Model once again.\n",
        "check_accuracy(trainloader, model)\n",
        "check_accuracy(testloader, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking accuracy on training data\n",
            "Got 57591 / 60000 with accuracy = 95.98\n",
            "Checking accuracy on test data\n",
            "Got 9608 / 10000 with accuracy = 96.08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i9kcUIg8Tsm"
      },
      "source": [
        "- Well thats some good score. So this was all about the basic RNN Implementation.\n",
        "# **THANK YOU!**"
      ]
    }
  ]
}