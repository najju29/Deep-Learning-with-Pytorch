{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN with PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZuG-c3AGwxv"
      },
      "source": [
        "# What is Recurring in Maths ?\n",
        "- Recurring in Maths is simply the repettition of a decimal or an expression.\n",
        "- A recurrence relation is an equation that defines a sequence based on a rule that gives the next term as a function of the previous term(s).\n",
        "- The simplest form of a recurrence relation is the case where the next term depends only on the immediately previous term.\n",
        "\n",
        "# Recurrent Neural Networks(RNN) in Deep Learning.\n",
        "- Recurrent Neural Networks (RNN) are a class of Artificial Neural Networks that can process a sequence of inputs in deep learning and retain its state while processing the next sequence of inputs.\n",
        "- Traditional neural networks will process an input and move onto the next one disregarding its sequence.\n",
        "\n",
        "# **How RNN works:**\n",
        "- <img src=\"https://www.researchgate.net/profile/Gunay-Abdullayeva-2/publication/342773701/figure/fig2/AS:911006222008322@1594212217359/An-unrolled-recurrent-neural-network-18.ppm\" width=\"800\" height=\"250\">\n",
        "- It uses previous information to affect later ones.\n",
        "-There are 3 layers: **Input(X), Output and Hidden(h)** (where the information is stored)\n",
        "- In forward step when an input is recieved into the cell it gets multiplied by an *initialized weight* and then it is been *send as an output* and then *used as an input in the next neuron*.\n",
        "- The output of other neurons gets weight initialization as well. Except the first neuron of each layer all other neurons get inputs from **two ends 1) from the previous neuron and 2) other as an input** i.e., next element in the data.\n",
        "- This is how any current neuron will have info of its current and previous data simultaneously.\n",
        "- *The loop*: passes the input forward sequentialy, while retaining information about it\n",
        "- This info is stored in the hidden state\n",
        "-There are only 3 matrixes (U, V, W) that contain weights as parameters. These DON'T change with the input, they stay the same through the entire sequence.\n",
        "- Here the information possesed by a neuron is passend on to the next one as well so that the new neuron will learn new pattern by keeping previous pattern in mind.\n",
        "- This is how the sequential information in the data is preserved.\n",
        "- Thus, It performs very good when it is given any sequential data of numeric or textual format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JBY08wO9SkC"
      },
      "source": [
        "- **NOTE:** Please refer to a more proper illustration about how RNN works before proceeding any further. The above ones are just simple tips about how different they are from the Artificial Neural Networks.\n",
        "- For Further info please go through this awesome blog:\n",
        "[RNN by Stanford](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks), and for video watch this simple illustration [RNN Illustration Video](https://www.youtube.com/watch?v=LHXXI4-IEns&t=496s&ab_channel=TheA.I.Hacker-MichaelPhiTheA.I.Hacker-MichaelPhi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc1q3L_YTZxi"
      },
      "source": [
        "# Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNDDckH5GuKH"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Reay1nUUDWg"
      },
      "source": [
        "# Let us Build a RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpj-VmVeUZVe"
      },
      "source": [
        "## Setting up **DATASET** and **HYPERPARAMETERS**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM5JpZrQUZHI"
      },
      "source": [
        "input_size = 28 # Image of 28x28 pixels which upon flattening becomes 784.\n",
        "sequence_length = 28\n",
        "num_of_layers = 2 # Number of RNN layers we want.\n",
        "hidden_size = 256 # Number of neuron per hidden state.\n",
        "num_classes = 10 # Number of classes in the Dataset are 10.\n",
        "learning_rate = 0.001 # Speed at which we want our optimizer to optimize our solution.\n",
        "batch_size = 64 # Size of the batch that will undergo training at one step.\n",
        "epochs = 2 # Steps of training or times a forward and backward propagation is done.\n",
        "\n",
        "# Firstly Loading a Data and downloading it to the folder.\n",
        "train_dataset = datasets.MNIST(root=\"content/\",train=True,\n",
        "                               transform=transforms.ToTensor(),download=True)\n",
        "# Now setting up its properties like batchsize\n",
        "trainloader = DataLoader(dataset=train_dataset,\n",
        "                         batch_size = batch_size,\n",
        "                         shuffle=True\n",
        "                         )\n",
        "\n",
        "# Doing the same for testset as well\n",
        "test_dataset = datasets.MNIST(root=\"content/\",train=False,\n",
        "                               transform=transforms.ToTensor(),download=True)\n",
        "\n",
        "testloader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size = batch_size,\n",
        "                         shuffle=True\n",
        "                         )"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpNGRYcBqMfT"
      },
      "source": [
        "## Creating RNN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKVX9o2aUZEI"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_of_layers, num_of_classes):\n",
        "    super(RNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_of_layers = num_of_layers\n",
        "    self.num_of_classes = num_of_classes\n",
        "    self.rnn = nn.RNN(input_size, hidden_size, num_of_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size*sequence_length, num_of_classes)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # Initializing hidden state.\n",
        "    h0 = torch.zeros(self.num_of_layers, x.size(0), self.hidden_size) # Initial Hidden State for each element in the batch.\n",
        "    # Forward step.\n",
        "    rnn_out_vector,_ = self.rnn(x, h0) # It takes in input vector followed by hidden state, returns and output vector along with hidden state.\n",
        "    rnn_out_vector = rnn_out_vector.reshape(rnn_out_vector.shape[0],-1)\n",
        "    output_from_linear_layer = self.fc(rnn_out_vector) # Sending in output of RNN into the Linear Layer(ANN)\n",
        "    return output_from_linear_layer\n"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E6wlTB3wfLk"
      },
      "source": [
        "# Initializing Model, Loss, Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRxQ3U1yUY-S"
      },
      "source": [
        "# Initializing the Model\n",
        "model = RNN(input_size, hidden_size, num_of_layers, num_classes)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6d05lMUGuGo"
      },
      "source": [
        "\n",
        "# Using CrossEntropyLoss as we have multiple classes.\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer as ADAM\n",
        "optimizer = optim.Adam(params=model.parameters(),lr=learning_rate)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIPtvxIMGnv5",
        "outputId": "65fcb1b5-a6d1-4915-cabf-47e91e549ef2"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  for batch_index, (data, targets) in enumerate(trainloader):# output of format = batch_index, (element_data, its_target)\n",
        "    data=data.squeeze(1)\n",
        "    # Forward Step\n",
        "     # Making Predictions on train_data\n",
        "    training_predictions = model(data)\n",
        "     # Calculating loss\n",
        "    Training_loss = loss_function(training_predictions, targets)\n",
        "    # Backward Step\n",
        "    optimizer.zero_grad()\n",
        "    Training_loss.backward()\n",
        "\n",
        "    # Optimizer Step\n",
        "    optimizer.step()\n",
        "  print(f'At {epoch} epochs Training_loss={Training_loss}')\n",
        "\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "At 0 epochs Training_loss=0.0071024359203875065\n",
            "At 1 epochs Training_loss=0.2920049726963043\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ndh9Hps0K2F"
      },
      "source": [
        "# Custom Function that calcultes accuracy of the Model.\n",
        "\n",
        "def check_accuracy(loader, model):\n",
        "    if loader.dataset.train:\n",
        "        print(\"Checking accuracy on training data\")\n",
        "    else:\n",
        "        print(\"Checking accuracy on test data\")\n",
        "\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.squeeze(1)\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "        print(f\"Got {num_correct} / {num_samples} with accuracy = {float(num_correct)/float(num_samples)*100:.2f}\")"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PcIBsvk7u5Z",
        "outputId": "4bf5a3b1-38d7-4505-987d-c4a34f7cc48f"
      },
      "source": [
        "model.train() # Training the Model once again.\n",
        "check_accuracy(trainloader, model)\n",
        "check_accuracy(testloader, model)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking accuracy on training data\n",
            "Got 57591 / 60000 with accuracy = 95.98\n",
            "Checking accuracy on test data\n",
            "Got 9608 / 10000 with accuracy = 96.08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i9kcUIg8Tsm"
      },
      "source": [
        "- Well thats some good score. So this was all about the basic RNN Implementation.\n",
        "# **THANK YOU!**"
      ]
    }
  ]
}
